{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2 training",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slippy0/gpt-2/blob/finetuning/gpt2_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dg0IvZq2pC2",
        "colab_type": "code",
        "outputId": "32b483c1-9939-463d-b52c-094e2c7480a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 297, done.\u001b[K\n",
            "remote: Total 297 (delta 0), reused 0 (delta 0), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (297/297), 4.39 MiB | 8.05 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n",
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxBnLU273OIS",
        "colab_type": "code",
        "outputId": "a2a4e916-55fd-4fa8-b6c2-cf2837abdfc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "%cd /content/gpt-2\n",
        "!pip install -r requirements.txt\n",
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n",
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyGZKRlMaONk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "\n",
        "# 117M or 345M\n",
        "model_name = \"117M\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDn0Yicq68UD",
        "colab_type": "code",
        "outputId": "85ec2103-7e7e-44a5-8f87-a8447719372a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!python gpt-2/download_model.py $model_name"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 899kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 57.8Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.09Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 66.5Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.60Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 53.5Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 48.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33DIf6LcceUp",
        "colab_type": "code",
        "outputId": "c7dcbee7-c49e-4889-e1c9-31cfad322282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# UPLOAD .txt FILES TO /content FIRST\n",
        "![ ! -d \"./data\" ] && mkdir data\n",
        "%mv *.txt data\n",
        "\n",
        "# Encode data\n",
        "!PYTHONPATH=gpt-2/src gpt-2/encode.py \\\n",
        "    --model_name $model_name \\\n",
        "    data encoded_data.npz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:00<00:00,  5.09it/s]\n",
            "Writing encoded_data.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwsm-MlD2q-7",
        "colab_type": "code",
        "outputId": "059a5505-b3a0-44dc-af22-378f68eeccbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3947
        }
      },
      "source": [
        "# This runs forever. Stop manually when you're getting good samples\n",
        "!PYTHONPATH=gpt-2/src gpt-2/train.py \\\n",
        "    --dataset encoded_data.npz \\\n",
        "    --model_name $model_name \\\n",
        "    --save_every 50 \\\n",
        "    --sample_every 50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-24 06:55:41.814643: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-05-24 06:55:41.814857: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f5c680 executing computations on platform Host. Devices:\n",
            "2019-05-24 06:55:41.814889: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-24 06:55:41.978256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-24 06:55:41.978763: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2f5bfa0 executing computations on platform CUDA. Devices:\n",
            "2019-05-24 06:55:41.978794: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-24 06:55:41.979176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-24 06:55:41.979210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-24 06:55:42.425079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-24 06:55:42.425158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-24 06:55:42.425171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-24 06:55:42.425449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Loading checkpoint checkpoint/run1/model-548\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00, 449.21it/s]\n",
            "dataset has 14690 tokens\n",
            "Training...\n",
            "2019-05-24 06:56:01.058023: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "[549 | 3.34] loss=0.82 avg=0.82\n",
            "Saving checkpoint/run1/model-550\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " View-In\n",
            "The MIX-ADVOC Preserved Network for Action Recognition\n",
            "AVA: Adaptive Image Accurate Rank Towards Similar Object Matrices\n",
            "Image-Image Translation: Alignment-Based Image Transformer\n",
            "CLEAR: Convolutional Neural Network for Less Memory?\n",
            "Multi-View Convolutional Neural Network for Less Memory?\n",
            "What Have We Learned From Deep Representations for Action Recognition?\n",
            "Leveraging Petri Dis for Faster Stereo Stereo\n",
            "Fitting a Stereo to a Robot's Bodies\n",
            "Multi-View Prediction for Hand Pose Optimization\n",
            "Learning Short-Term Consistency Motion From Videos by Identifying Weakly-Supervised Features\n",
            "Towards Effective End-to-End Training ofGANs\n",
            "GAN Basics for Multiple Vision Recognition\n",
            "Learning a Discriminative Filter Chain for Semantic Segmentation\n",
            "GAN-Style Sparse Generative Adversarial Networks\n",
            "Learning a Discriminative Group-Time Learning for Vision Recognition\n",
            "Geometric Shape Completion With Regularity and Consistency in Perspective Clustering\n",
            "Regularizing Pose and Facing That Composition Constraint for Hand Size-Aware Joint Size-Batching\n",
            "Squeeze Clustering and Robustness for Robustness Based Face Detector\n",
            "Pixored Face Detector and Human Body Map\n",
            "Weakly-Supervised Feature Learning for Stereo and Human Audience Categorization\n",
            "Learning and Using the Square Root of Sea-Wax Cut-Point\n",
            "Weakly-Supervised Semantic Segmentation by Learning Object Class From Video Streams\n",
            "Attention-Aware Feature Learning for Stereo and Human Audience Categorization\n",
            "Learned Feature on Person Re-Identification\n",
            "Direction Recognition by Referring Forward Person Re-Identification\n",
            "InverseFace: Learning Distantly-Wade Class Depth With Inline-Aware Diverse Grouping\n",
            "Learning Diverse Dense Neural Regions for Pose-Guided Face Recognition\n",
            "FaceIllumination: Learning Deep Faces From the Outside Out\n",
            "Look, Imagine and Match: Adapting Feature Maps to Geometric and Grammar Conditions\n",
            "Differential Domain Adaptation for Action Recognition\n",
            "Visual Question Answering With Domain Adaptation\n",
            "Neural Network for Sequential Task Segmentation\n",
            "Recovering Images With Loss Made for ImageJolt\n",
            "Action Recognition With Phrase-Based Domain Adaptation\n",
            "Geometric Domain Adaptation for Person Re-Identification\n",
            "Neural Online Camera Ranking\n",
            "Multi-View Action Recognition With Alternative Feature Maps\n",
            "Disentangling Factors at the Bi- and Person-Eye-Eye Level\n",
            "Robust Person Re-Identification With Multi-View Zoom-In and View-QA\n",
            "Sliced Neurons in the Wild: Learning All the Surfaces\n",
            "Learning Weakly-Supervised Feature Learning\n",
            "Recovering From An Unrecognized Thing\n",
            "Learning Feature-Set Annotation for Learning Single-Image Models\n",
            "Feature Adaptation for Person Re-Identification\n",
            "Going From Weakly Supervised Feature Learning To Strongly-Supervised Learning\n",
            "In-Place Activated Fusing? A Common Approach for Multi-Person Segmentation\n",
            "Learning Complex Features From Crowds With Instance Registration\n",
            "Learning Complex Features From Crowds Without Instance Registration\n",
            "Disentangling Factors at the Region Transfer (RTransfer) Level\n",
            "Learning Compressible 360° Video Maps\n",
            "Learning Multi-View Zoom-In and Zoom-Out Experiments With Semi-Progressive Zoom-In and 2D Outside View\n",
            "Multi-View Feature Learning With Part-Timer Inference\n",
            "Learning Face Morphable Maps With Depth-Based Part Detection\n",
            "Learning Multi-View Feature Using Conditional Appearance Feature\n",
            "DenseCorrelation Learning Complex Face Segmentation With Distant Correlation\n",
            "Seeing Small Movements in Social Scenes With Momentum\n",
            "Towards Learning Complex Social Scene Observances\n",
            "Rotation-Empirical Study of Pose and Pose-Elevation From a Shot-to-Shot Display\n",
            "Rotation-Empirical Study of Single View Single Object Object Classification\n",
            "Scale-Solving the l1-Normative Face Detector for Depth\n",
            "TextureShift: Extracting Texture and Partitioning Images Of Any Style From a Single Image\n",
            "Cross-View Stepwise View Mat for Multi-View Room Environments\n",
            "Left-Right Semantic Matching via a Single Network of Spatial Constraints\n",
            "Image Intrinsification by Iterative Right-Left Alignment\n",
            "Image Compression via a Stacked CNN Datatol Group\n",
            "Inferring the Weak Visual Discovery in Surveillance Videos <|endoftext|>\n",
            "What Makes a Video a Video Demonstrable Life-cycle Visual?\n",
            "Image Intrusion Detection Through Reinforcement Learning <|endoftext|>\n",
            "Image Retrieval With Structured Types and Outside Variables <|endoftext|>\n",
            "Correlation Parsing for Video Encoder Generalization\n",
            "Fast Video Intrusion\n",
            "\n",
            "[550 | 19.21] loss=0.47 avg=0.65\n",
            "[551 | 19.68] loss=0.40 avg=0.56\n",
            "[552 | 20.14] loss=0.72 avg=0.60\n",
            "[553 | 20.62] loss=0.68 avg=0.62\n",
            "[554 | 21.08] loss=0.58 avg=0.61\n",
            "[555 | 21.54] loss=0.51 avg=0.60\n",
            "[556 | 22.01] loss=0.46 avg=0.58\n",
            "[557 | 22.47] loss=0.46 avg=0.57\n",
            "[558 | 22.94] loss=0.43 avg=0.55\n",
            "[559 | 23.41] loss=0.39 avg=0.54\n",
            "[560 | 23.88] loss=0.34 avg=0.52\n",
            "[561 | 24.34] loss=0.45 avg=0.51\n",
            "[562 | 24.81] loss=0.59 avg=0.52\n",
            "[563 | 25.28] loss=0.46 avg=0.52\n",
            "[564 | 25.75] loss=0.54 avg=0.52\n",
            "[565 | 26.22] loss=0.56 avg=0.52\n",
            "[566 | 26.68] loss=0.57 avg=0.52\n",
            "[567 | 27.15] loss=0.56 avg=0.53\n",
            "[568 | 27.61] loss=0.48 avg=0.52\n",
            "[569 | 28.08] loss=0.43 avg=0.52\n",
            "[570 | 28.55] loss=0.34 avg=0.51\n",
            "[571 | 29.02] loss=0.43 avg=0.51\n",
            "[572 | 29.49] loss=0.38 avg=0.50\n",
            "[573 | 29.96] loss=0.37 avg=0.49\n",
            "[574 | 30.43] loss=0.48 avg=0.49\n",
            "[575 | 30.89] loss=0.46 avg=0.49\n",
            "[576 | 31.36] loss=0.39 avg=0.49\n",
            "[577 | 31.83] loss=0.40 avg=0.48\n",
            "[578 | 32.31] loss=0.54 avg=0.49\n",
            "[579 | 32.78] loss=0.30 avg=0.48\n",
            "[580 | 33.25] loss=0.39 avg=0.48\n",
            "[581 | 33.72] loss=0.45 avg=0.47\n",
            "[582 | 34.20] loss=0.48 avg=0.47\n",
            "[583 | 34.67] loss=0.36 avg=0.47\n",
            "[584 | 35.14] loss=0.49 avg=0.47\n",
            "[585 | 35.61] loss=0.46 avg=0.47\n",
            "[586 | 36.07] loss=0.41 avg=0.47\n",
            "[587 | 36.54] loss=0.60 avg=0.47\n",
            "[588 | 37.01] loss=0.47 avg=0.47\n",
            "[589 | 37.48] loss=0.43 avg=0.47\n",
            "[590 | 37.95] loss=0.33 avg=0.47\n",
            "[591 | 38.42] loss=0.26 avg=0.46\n",
            "[592 | 38.89] loss=0.36 avg=0.46\n",
            "[593 | 39.36] loss=0.44 avg=0.46\n",
            "[594 | 39.83] loss=0.38 avg=0.46\n",
            "[595 | 40.30] loss=0.37 avg=0.45\n",
            "[596 | 40.77] loss=0.32 avg=0.45\n",
            "[597 | 41.24] loss=0.32 avg=0.45\n",
            "[598 | 41.71] loss=0.45 avg=0.45\n",
            "[599 | 42.18] loss=0.32 avg=0.44\n",
            "Saving checkpoint/run1/model-600\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "Shot System, 3D-BOX: 3D-CBM-Aware Parametric 3D-CBM-Nonconvex Parametric 3D-CBM-Unformable Conditional Captive Learning <|endoftext|>\n",
            "Learning to Adapt General Attention to Nonconvex Problems <|endoftext|>\n",
            "Fast Joint Retrieval on a Pixel-Wise CNN for Image Compressive Sensing <|endoftext|>\n",
            "Zero-Shot: Refinement-and-Reliability Enhancer in Convolutional Schemas\n",
            "Bidirectional Key-Value Convolutional Semantic Embeddings\n",
            "Context-Aware Lossy to Cuts for Image Super-Resolution\n",
            "Learning EnvTLR via Deep Boundary Residual Networks\n",
            "On the Integration of Network-Aware Feature Compression and Density Stereo Matching <|endoftext|>\n",
            "Avatar-Net: Deep Layer Invariant Divx Matching\n",
            "Making Convolutional Uncertain Curves With Block Curves\n",
            "MaskLab: Learning Convex Shape Labeled Transformer From Video\n",
            "PackNet: Learning Shape Predictor via Large-Scale Variational Covariance Pooling\n",
            "Local Descriptors Optimize Feature Branching\n",
            "Densely Connected Cross-Spectral Beam Lines Network\n",
            "Learning Compositional Directionality for Visual Points Tracking <|endoftext|>\n",
            "LabelShift: Embedding Sense for Descriptable Categories <|endoftext|>\n",
            "A Fast Dense Photograph With Fewest Cuts <|endoftext|>\n",
            "Analyzing Motion Images as Actions Whereas Hallelic Priors Explain Few Common Features\n",
            "Defense Against Adversarial Attacks Using Weakly Supervised Adversarial Programming <|endoftext|>\n",
            "Learning Weakly Supervised Semantic Grammar on C++14 and C++15 Standardization Clustering\n",
            "Towards Global Saccade-Free Image Captioning <|endoftext|>\n",
            "Generating Globally Complementary GeoNet for Visual Points Tracking <|endoftext|>\n",
            "Context-Aware Hierarchical Hierarchical Regression for Point Cloud Sketch Detection <|endoftext|>\n",
            "A Uncertainty-Aware Latent Super-Shot Model for Handling Motion Inertial Fields <|endoftext|>\n",
            "A Uncertainty-Aware Latent Point Cloud LatMap <|endoftext|>\n",
            "An Unsupervised High-Speed Render Using Simultaneous Light Maps and Observant Temporal Condences <|endoftext|>\n",
            "Weakly-Supervised Linear Views Using Discrepancy for Fast Response\n",
            "Efficient, End-to-End Learning of Video Object Segmentation <|endoftext|>\n",
            "Geometry Network for Zero-Shot Learning With Extreme Attention\n",
            "Zero-Shot Sketch Detection With Convolutional Symmetric Sampling and Non-RNN Models <|endoftext|>\n",
            "Learning Zero-Shot Sketch Localization With Regression Learning <|endoftext|>\n",
            "Multi-Shot Stacked Condensory Videos Using Sparse Convolutional Neural Networks <|endoftext|>\n",
            "Progressive Attention Compression for Zero-Shot Learning <|endoftext|>\n",
            "Zero-Shot Sketch Localization With Network-Aware Convolutions and MaximDuPads <|endoftext|>\n",
            "Learning Few-Shot Adaptive Similarity Learning by Learning to Sum-Multiply <|endoftext|>\n",
            "Learning One-Shot Image Scale With Weakly-Supervised Learning\n",
            "Zero-Shot Adaptive Similarity Learning With Domain Adaptation for Visual Question Answering <|endoftext|>\n",
            "Learning One Image Model Fine-Pass Snippet for Image Classification\n",
            "Learning One Millionth Sem\n",
            "Squeeze-and-Sketch: Learning Flexible and Accurate Riemannian Motion Segmentation With Deep Learning\n",
            "Multi-Shot Segmentation With Domain Adaptive Similarity\n",
            "Recurrent Saliency as Visual Saliency Estimation\n",
            "Learning Complex Behavings From Segmentations Without Segmentation Knowledgeblowing\n",
            "Learning Complex Generative Adversarial Networks for Object Detection\n",
            "Detect-and-Semit-Sensitive-Recalibrating-Through-Answering-WITH-AUSTRATIVE-ADJUST\n",
            "W2-AVRL-Net : Learning Wasserstein Uncertainty's Proper Constraint Loss\n",
            "Hierarchical Attentionalization by Repeated Up-Converted-Down Convolutional Networks\n",
            "Deep Sparse Coding for Memory Science\n",
            "Fast On The Rock, Rich Bland Outrageously Fast Forwarded Hack Expands Its Utility\n",
            "CRRNate: A Naturalistic Approach to RobustRecall\n",
            "CRRN : A Novel Method for Decoding Deep Networks\n",
            "CRRNome : Robust Recurrent Image Classification With CRISP2\n",
            "CRIME-Aware Guided Prediction-\n",
            "\n",
            "[600 | 56.38] loss=0.39 avg=0.44\n",
            "[601 | 56.86] loss=0.36 avg=0.44\n",
            "[602 | 57.32] loss=0.22 avg=0.44\n",
            "[603 | 57.80] loss=0.38 avg=0.43\n",
            "[604 | 58.27] loss=0.32 avg=0.43\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3t_K4vDf_zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate_samples is hardcoded to this path, so copy our trained models there\n",
        "!cp -r /content/checkpoint/run1/* /content/models/$model_name/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUhrXPWawPp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a random sample\n",
        "!PYTHONPATH=gpt-2/src python gpt-2/src/generate_unconditional_samples.py \\\n",
        "    --model_name $model_name \\\n",
        "    --nsamples 1 \\\n",
        "    --top_p 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjQG6bsEzEdW",
        "colab_type": "code",
        "outputId": "fa354122-1973-4237-f83c-0e1dae222689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5426
        }
      },
      "source": [
        "# Generate samples with a prefix\n",
        "!PYTHONPATH=gpt-2/src python gpt-2/src/interactive_conditional_samples.py \\\n",
        "    --model_name $model_name \\\n",
        "    --top_p 0.9"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-24 06:58:38.931167: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-05-24 06:58:38.931427: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x306c520 executing computations on platform Host. Devices:\n",
            "2019-05-24 06:58:38.931461: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-05-24 06:58:39.090166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-05-24 06:58:39.090659: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x306be40 executing computations on platform CUDA. Devices:\n",
            "2019-05-24 06:58:39.090688: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-05-24 06:58:39.091007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-05-24 06:58:39.091029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-05-24 06:58:39.544881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-05-24 06:58:39.544950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-05-24 06:58:39.544963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-05-24 06:58:39.545230: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-05-24 06:58:39.545277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> CATGAN: \n",
            "2019-05-24 06:58:51.275545: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Arbitrary Catalytic Convolutional Neural Networks\n",
            "Cascade: Cascaded Neighbors for Small Numbers of Images\n",
            "Wrapped Networks: WGANs for Sketch-and-Recovering From An Image View\n",
            "SketchNet: Training a Crowd-Convolutional Model for Zero-Shot Therapy\n",
            "AnalysisNet: A Fast and Accurate Way to Find Trajectories in Deep Neural Networks\n",
            "Neural ChemCam: A Large-Scale Neat PhotoScan Capture Using ChemCam, OpenCL and OpenGator\n",
            "Efficient Reconstructive Scan of Images for Intelligent Identification\n",
            "Leftover Shadow Occlusion for Visual-Inertial Image Synthesis\n",
            "Real-Time Reconstruction and Enhancement of HDR Images by Deep PhotoScanner Inference Processor\n",
            "End-to-End Automatic Resizing and Embedding of Gaussian Cropping Networks\n",
            "ISRN: International Standard for Reference Number Translation\n",
            "DenseStructuredStreams: Deep Structured Inline Object Recognition\n",
            "CBM: Complete Boundary Matching Modeling\n",
            "Image Fixation Without a Frame: An Inference Graph for Style-Correct Method Synthesis\n",
            "Deflateak: Unsupervised Blazing at Monuments Size A and B, Too Many for Image Sizing\n",
            "Level 3M Bases Using Deep Field Networks\n",
            "Single-Shot Image Style Estimation via Continuous Modal Generation\n",
            "Temporal Gaussian Blazing\n",
            "Discovering Zero-Shot Style Descriptors by Dual Attention Policy With Sparse Convolutions\n",
            "Discrete-Invariant Image Quality Estimation With RGB-D Videos Learned From Local Temporal Regressions\n",
            "Multi-Scale Long-Shot Image Quality Improvement by Reinforcement Learning of Large-Scale Attention Networks\n",
            "Task-Aware Image Captioning: Learned From Video-ASSM Programs\n",
            "Learning Deep Image Captioning via Competitive Reinforcement Learning of Uncertainty About Image Object Class\n",
            "Jerk-Aware Image Captioning: Learning From Unlearnable Loss Scenes\n",
            "Deep Automatic Image Object Class\n",
            "Few-Shot Image Quality Improvement\n",
            "Structure-Aware Image Captioning\n",
            "Pose Transfer Learning for Small Unit Image Transfer\n",
            "Show Me a Story by Generative Adversarial Networks\n",
            "Structured Captions Using Captive Catastrophic Correlation Learning\n",
            "Learning Structured Captions Using Conditional Rank Encodings for All Purpose Subrogression Prediction\n",
            "Deep Frame-Based Image Compressive-to-Varying Convergence\n",
            "Towards High Performance Video Completion With Infinite-Scale Stereo Gradients\n",
            "================================================================================\n",
            "Model prompt >>> KITTYGAN: \n",
            "======================================== SAMPLE 1 ========================================\n",
            " Human-Centric Augment Regression for Image Recognition via a Multi-Target Loss Function\n",
            "WILD DIFFERENTANTS: A Synthesis and Application Study of Dualized Loss Functions for Image-Co-Attendency Domain Learning\n",
            "LOSE-LOSE: Dynamic Lifestyle-Specific Weight Loss With Random Walk and Eye Look-Atner Presently Perturbed?\n",
            "Super-Resolving the Blind Spot in Vanishing Point AnImage: Just What You Need to Enrich Human Memory in the End - Without Labels - and Understand Why We Like Them So Much\n",
            "BIDDLE: Building a Fast and Decentralized Neural Network Based on Open Text Kratios\n",
            "Plane Through 250 Images of Self-Driving Driving Without Pedestrian Lines\n",
            "VOCABOLAT: A Low Power, High Temperature and Relative to a Layer Map Sensor Do Not Disturb Under Skin Lighting <|endoftext|PLA3200>\n",
            "CTRL-Pro: Small Capsule Improves Dynamic Effectiveness in Traffic De--cross-contamination (DTCAO) †Study Finds An Effective Pedestrian Detection Rate †Conditional De-Cross-Modulation Mechanism for Traffic De-Cross-Detection <|endoftext|DOTA0030>\n",
            "PUP: Quantum Point Super-Resolution for 3D Object Detection <|endoftext|PANSPROC>)\n",
            "Towards Intrinsic 3D QA: A Single Square Kilometer Pooled Region Is A Triplety\n",
            "CH4236: Is That a Person Still Is? †Intersection and Matching Is Metricly</|endoftext|CH4236>\n",
            "Deep Matching Based on Scale Completion †Single-Object Multi-Target Matching Algorithm</|endoftext|CH4236>\n",
            "Context-Aware Subspace Pooling for Scene Text Recognition †Joint Pooling Regression and Unified Markov Reciprocating Task</|endoftext|CAMSP>\n",
            "Learning From Videos With Attentional Cues ‡Attention-Aware Region Consistency for Face Completion and Semantic Segmentation in Videos <|endoftext|TESS3002>\n",
            "Efficient Feature Compensation for Scene Text Detection ‡View Completion and View Semanticization Preserved Using a Multi-Scale Feature Pooling Algorithm <|endoftext|QA006>\n",
            "Visual Question\n",
            "================================================================================\n",
            "Model prompt >>> DOGGAN: \n",
            "======================================== SAMPLE 1 ========================================\n",
            "𝔈 Tensor for Point Cloud Estimation via Feature Space Deletion\n",
            "Kernelized Point Cloud Estimation With Semantic Branches\n",
            "Optimizing Point Cloud Graph Prediction With Hierarchical Regression Prediction Networks\n",
            "FIT-FCN: Towards Augmenting Point Cloud Datasets With Gaussian Corroutines\n",
            "Progressive Point Cloud Reinforcement Learning for Flexible Visual Network Learning\n",
            "Learning Deep Points From Clumps With Trajectories\n",
            "3D Hashing for Dense Rolling Capsule Scenes in Cinema\n",
            "Morph: Deep Morphling for Polygonal Combustion\n",
            "Single-Shot Visual Document Retrieval With Stacked Orthogonal Fields\n",
            "PoseFlow: Learning Stacked Inference for Single Image Flow Through Point Cloud Estimation\n",
            "Improved Depth of Field in HDR Video Scenes With Gaussian Fields\n",
            "3D Pose Sequencer for Motion Expressions in 3D Bases\n",
            "Zero-Shot Learning in Filmmaker-Less Cameras on Surface Contours\n",
            "A 3D Segmentation of Objects in Real-World Traffic by Hand\n",
            "Audio-Aware Kernelized Point Cloud Prediction Using Grammar-Recurrent Neural Networks\n",
            "SPACE: Predicting and Describing the End-of-Day?\n",
            "Stepwise R-CNN for Point Cloud Estimation in the Wild\n",
            "End-to-End Video Reasoning for Discovery of Dog-Pig Shape in a Geometric Poses\n",
            "Arbitrary-Number-Solving Instructions for a Domain-Based Neural Net\n",
            "Learning a Discriminative Narrative From a Single Image\n",
            "DenseMulti-ShotNet: A Novel Neural Net for Point Cloud Estimation in the Wild\n",
            "PoseFlow: A Diverse, Fast and Optimal Method for Point Cloud Estimation Using Trajectories, Recurrents and GANs\n",
            "3D Pose Estimation With Dense Network for Antialiasing in Adobe Illustrator\n",
            "Low-Shot Learning With Uncertainty\n",
            "Feature Indicator Detection With Normalized Priors\n",
            "Progressive Weighting of Deep Network Features\n",
            "Depth, Optimal Depth for Feature Deletion and Noisy Labeling: A Perturbative Approach\n",
            "Structured Optimal Deep Neural Network for Fashion Fashion Classification\n",
            "Deflecting Adversarial Attacks With Dynamic Size Padding\n",
            "DenseBin: Light-Weight Photometric Dendritic Transformation Undergoing Morphial Attention\n",
            "Multi-Scale Batch Patching for Minimalist Fashion Models\n",
            "In-Place Self-Supervised Light Field Learning Using Ad\n",
            "================================================================================\n",
            "Model prompt >>> DOTA: \n",
            "======================================== SAMPLE 1 ========================================\n",
            "க_க்ூய்\" - Wikipedia:ArbitraryAdversarialNet\n",
            "GADGET: Generalized GAN for Subspace Domain Adaptation\n",
            "Rotation Denoising Adversarial Networks\n",
            "WebSLAM: A Complete Loss Course With High Quality Trajectories\n",
            "Divide and Conquer: Lossy Video Caption Adaptation With Subspace Model Classification and Graph Optimization in Subspace-Aware Model Connections »\n",
            "Learning Deep Network With Bundle Representation With Receiver Network\n",
            "Multi-Task Perceptual Learning With Weakly Supervised Discovery\n",
            "G-Residual Networks : How Adversarial Models Are Enabling Deep Learning on Millions of Text Images\n",
            "Deep Font Filter: Generating Uniquely Tic-Hive 3D Pose Estimation in the Wild\n",
            "The PowerHOU: High-Speed High Level Instance Fusion Using Instance Activation With Hashing-Coupled Deep Embedding <|endoftext|>\n",
            "Single Image Caption Adaptation With Activation- or Point Blank? Q&A <|endoftext|>\n",
            "Compare and Contrast: Deep Mutual Learning <|endoftext|>\n",
            "Joint Attention and Visual Attention for Person and Object Image Adaptation <|endoftext|>\n",
            "Zero-Shot Learning With Convolutional Neural Networks for Visual Question Answering <|endoftext|>\n",
            "A Memory-Driven Approach to Visual Question Answering <|endoftext|>\n",
            "A Methodical Study of Rank-Scale Visual Attention, With and Without Constraints <|endoftext|>\n",
            "Social Scene Attribute Learning <|endoftext|>\n",
            "Geometry-Aware Feature Completion for Visibility <|endoftext|>\n",
            "A Closer Look at Raindrops on a Large Scale <|endoftext|>\n",
            "Generative Modeling of Low- and Fast-Methane Scenes <|endoftext|>\n",
            "A Variational U-Squared for Object Angle Calibration <|endoftext|>\n",
            "Superlative Attention for Visibility Understanding Spatio-Temporal Data <|endoftext|>\n",
            "Optimal Angle Calibration for Video Renderings <|endoftext|>\n",
            "Generative Hyperspectral Image Recognition <|endoftext|>\n",
            "End-to-End Reasoning in Context Images <|endoftext|>\n",
            "Learning to See in Images That\n",
            "================================================================================\n",
            "Model prompt >>> DOTA: \n",
            "======================================== SAMPLE 1 ========================================\n",
            "한다 한국어\n",
            "MAPK: Mapping Shape Features to Human Features\n",
            "Learning to Estimate Co-Occurrence Maps Using Principal Image Analysis\n",
            "Dimensionality-Driven Ground-to-Anchors Map Generation With Kernel Correlation and Gradient Indoorization\n",
            "Hierarchical Depth Segmentation of Structured Space via Multiple Tasks and Deep Localizations\n",
            "Revisiting Scale-Time Question Constraint: A High Level Algorithm for Point Based Localization\n",
            "Demo 3D Reconstruction of a 3D Human Head Under Uncertainty In A Classroom Model\n",
            "Neural Network Convolutional Neural Network: Combining Real-world experience and intuition to design deeper and more intuitive connections\n",
            "PPO: Power Paid for Out-of-FIT Image Opti-Tagging\n",
            "Learning Compositional Depth Labeled Contours In The Domain's Best View HDR Filter\n",
            "DOTA-GAN: Adding Light Deblurring To Images Convoluted From Blind Parts\n",
            "Recurrent Pooling Network for Multi-Person Scene Visualization\n",
            "Clean Room and Face Restoration With Deep Room Analysis\n",
            "VITO: Video tour de force: Iterative VITO reconstruction by the use of promise and demand\n",
            "Augmenting Object Classes With Atomic Pooling Corridors\n",
            "Minimalist Method 3D Object Renderer\n",
            "Modeling Topology With Point Cloud Latent Lights\n",
            "Visual Basic of a Cinematic World With Hallucinated Normal Maps\n",
            "From CRF Networks to Interactive Narrative Estimation: A Perceptual Experiment\n",
            "TIFF: Transparent High Density Detailed Image Set Retrieval With TIFF-KC\n",
            "Enhancing Textual Body Localization With Guided Attention\n",
            "Non-Blind Spatial Unsupervised Learning for Object Detection\n",
            "Mappings of People and Object via a Single Image-to-Image Transfer\n",
            "Generalized Zero-Shot Learning\n",
            "The Art of Quotient Interactions\n",
            "Adversarly Field Networks for Aerial Vehicle Segmentation\n",
            "Learning to Generate Time Discrete Affordances From Display Surfaces\n",
            "Robust Person Re-Identification With Robust Person Re-Distributions\n",
            "Learning to Interpret Video Evidence as Directly Sent Actions Involved\n",
            "The Importance of Tracking Biases in Joint Groundings\n",
            "Fast and Furious: Robust Handling of a Deadly Incident Is Beyond the Call of Duty Standards\n",
            "Im4U: Understanding Nonconvex Methods Without Instance Change\n",
            "Feature Completion for Aerial Vehicle Segmentation\n",
            "================================================================================\n",
            "Model prompt >>> Deep Reinforcement Learning for\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Machine Learning\n",
            "4D Camera Reconstruction via Adaptive Face Refinement\n",
            "Empirical Study of Person Re-Attraction in a 3D Face\n",
            "Structure Driven Feature Translation for Person Re-Attraction in a 3D Object View\n",
            "A Delayed and Decoupled Neural Response Path Network Explores the Decision-Making Power of Pipelines\n",
            "Robust Structure Discovery Using Model Transformer Learning\n",
            "Person Re-Attraction Using Dominated Attention in Discovery\n",
            "Discrepancy Residual-AVA Networks\n",
            "Towards Universalized Person Re-Attraction\n",
            "Revisiting Recurrent Representation for Higher-Order Complementary Transformations\n",
            "Learning a Model-CAN Textured Robustness Network\n",
            "MaskShift for Fast Resection-of-width Loss\n",
            "The Neural Deep Web: No-Drop Lifting Margin for Online Game Scenes\n",
            "ModClothRotationManIP: Pruning Lights in Soft Structures With Guided Propulsion for 360-Pounding Super-Resolution\n",
            "CloMoIP: Manipulating Shape Derivatives With a Single Layer of Deep Learning\n",
            "Progressive Perturbant Removal of ABA-GDOMatiques With Sparse Block Morphisms in OpenCL GANs\n",
            "Deep Refinement on Kendall's Law for Depth Super-Resolution\n",
            "DoubleGoodVM: Watching a Mannequin Motion Guided By Its Camera\n",
            "Virtual Hand Segmentation: Instance Compression With Localization In Virtual Machine\n",
            "MemoryLP: Instance Compression With Deep Lifting Rules\n",
            "Egocentric Inference With Long-Term Norm Tracking\n",
            "Single-Shot Depth and Acceptance Optimization for Optical Flow Estimation\n",
            "Egocentric Super-Resolution: Instance Compression Without Occurrance\n",
            "Video Pedestrian Detection From a Single BitTorrent Video Stream\n",
            "Learning to Detect When Someone Is Looking In the Wider Wider Slope\n",
            "Pose Recognition via Neural Networks\n",
            "Handcuffed Manifold: Perception-to-Direction Matching in Manifold Games\n",
            "Continuous Selective Networks for Context-Aware Scene Text Recognition\n",
            "Attentive Neural Face Detection via Ordinal Regression Networks\n",
            "Translating Dialogs for Semantic Segmentation\n",
            "PointNetwork: Detecting Dialogs in Images of Two People\n",
            "Smooth Face Recognition on Organ Facial Context\n",
            "Human Pose Invariant Embodiments: Learning Pose Estimation With Adversarial Images\n",
            "Geometry-Aware Semantic Mask Reshuffle With Advers\n",
            "================================================================================\n",
            "Model prompt >>> Gender Aware\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Parenting\n",
            "Efficient and Accurate Metric Random-Axis for Multi-Person Labeling\n",
            "Unsupervised Attention: Sub-Gaussian Uncertainty in Enriched Learning\n",
            "Mining Salient Objects: Distortion-Free Mining of Salient Object Images From a Single Image\n",
            "The Invisible Square: 3D Density Normalized With Diffuse Surfaces\n",
            "Depth-Aware Feature Reshuffle With Super-Fast Cameras\n",
            "Recurrent Neural Lab: Unsupervised Learning Over Zero-Shot Training\n",
            "Bottom-Upshoot Transformer Lab: One More Thing That Can Be Done: More Deep!'s' No need for Zero-Shot Learning\n",
            "Frustratingly Quick: Deep Instance-Level Prior for Embedded Neural Information Transfer\n",
            "Super-Resolution: Instance-Level Transfer With No Time Inbetween\n",
            "Custom Learning With No Refinement\n",
            "Multi-Scale Convolutions: No More Instance-Level Curl Moments\n",
            "Tracking Convolutions: Instance-Level Curl With Deep Neural Network With No Lab\n",
            "Image-in-picture Detection: No-Image Stereo Surrogate Disentrement\n",
            "Creating Attentive Videos From Snippets as Videos From Parts: Attentive Video Synthesis and Synthesis With Kernelized Transformer Network\n",
            "DistorttoFo: Refinement-Aware Synthesis Language for Video Completion\n",
            "Generative Modeling of Mobile Devices: Learned Abnormalities\n",
            "Losele's Rotation-Preserved Model for Reproducing Short Symmetry Images\n",
            "Hierarchical Spatio-Temporal Representations for Video Assorted Interaction\n",
            "Vision-Learning: Transferable Attentional Focus Without Compounding Learning\n",
            "Zipping Cartesian Boundary: No-Swinging Reasoning for Co-Occurrence Learning\n",
            "Learning via Discovery | No-Swinging Reasoning on a Budget\n",
            "Image Editing With Sequential Decision Making and Attention\n",
            "Interactive Image Compression: Alternatives to Adversarial Learning\n",
            "Improving Image Quality in the Wild by Learning From Past Findings\n",
            "Semantics-Preserving Internal Decorrelation Learning\n",
            "Tracking Translations for Semantic Segmentation: Learnings From Everyday Activities\n",
            "Augment Methods With Uncertainty: Parallax Not Instruments Not Critical Complement\n",
            "Fast Underwater Embedding Learning With InTAmNet\n",
            "Unsupervised Embedding Translation With Cauchy RGB-CbNet\n",
            "Deep Image Completion: Unsupervised Learning Made Simple\n",
            "\n",
            "================================================================================\n",
            "Model prompt >>> Harder Better Faster: Going Deeper to\n",
            "======================================== SAMPLE 1 ========================================\n",
            " Improve Performance\n",
            "Tighter and Longer Matching Regressions: Optimized Transfer Rate for Higher-Order Output Manipulation\n",
            "From Online Shading to Object Detection, How People Gaze — and Do — More Complexly\n",
            "Video Reasoning Using Part Maps to Reason and Reason Back\n",
            "Unsupervised Learning to Large Scale Object Detection via Denoising Color Fields and Denoising Radians\n",
            "Good Looking Images Progression From Motion to Image Quality\n",
            "The Power and Dilution of In-Place Generative Adversarial Networks for Weakly Supervised Action Localisation\n",
            "Hashing as Shape Transfer Learning\n",
            "Learning to Spot Rain Sluts With Temporal Convolutional Nets\n",
            "Multi-Target Dilution Control by Feature Autoencoders for Weakly Supervised Action Localisation\n",
            "Photometric Synthesis With Diluted Photometric results\n",
            "DenseLiteFlow: Storing, analyzing and displaying 3D models in watercolor\n",
            "Structure Based Guided Learning for Light Transport Modeling\n",
            "Convolutional Fusion: Generating good-looking 3D models by splitting the world\n",
            "Robust Facial Landmark Detection With Binary Fields and Gaussian Patches\n",
            "TransferRate's Verylocal Capsule Learning\n",
            "Where and Why Are They Looking?\n",
            "Automatic Enrichment of Visual Shortcut Tracking With Loop Signaling\n",
            "Avatar-Net: Fast and Accurate Filament Obscenity Matching by Motion and Volume\n",
            "A Domain-Specific CNN for Robust Object Tracking in Search-and-Smear Videos\n",
            "High Performance Texture Decomposition Using SPIN on Point Cloud Mesh Collage Research\n",
            "Motion- and Light-Level Interactions in VR-Mode: Propagation, Ray-Recall and Recovery Time\n",
            "Finding Shape Interactions in Articulated Clouds and Gels Without Transformation at All Clusters\n",
            "PoseFlow: A Simple Approach for Robust Depth-Aware Object Segmentation With Geometry With Abstraction\n",
            "Learning Unconstrained Pose Estimation With Linear Gradient Without Subtraction\n",
            "Time Warp: One Body, One Moment for Single Action Time-Lapse Videos\n",
            "High Level Reference Learning for VideoConv\n",
            "SofiegoLS: Learning Single-Image Super-Resolution Single-Image GCode With Concurrent Pooling in Weighing Video and Audio Videos\n",
            "Multi-Image Fixation and Mark Synthesis in CNNs Using Synthetic Data\n",
            "Fast Fourier Active Transformer for Unpaired Light Field Deceleration\n",
            "Crafting's Fixation Model for Un\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"gpt-2/src/interactive_conditional_samples.py\", line 71, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"gpt-2/src/interactive_conditional_samples.py\", line 89, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"gpt-2/src/interactive_conditional_samples.py\", line 86, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rFwORnJ4AEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove everything to start over\n",
        "%rm -rf checkpoint\n",
        "%rm -rf data\n",
        "%rm -rf gpt-2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}